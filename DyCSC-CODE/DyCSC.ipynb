{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdacde9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZHANGSHANFAN\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "EPS = 1e-15\n",
    "from torch.nn import Parameter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "device = 'cpu'\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd5e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZHANGSHANFAN\\AppData\\Local\\Temp\\ipykernel_5468\\2292249624.py:2: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  adj_time_list = pickle.load(handle,encoding='iso-8859-1')\n"
     ]
    }
   ],
   "source": [
    "#with open('data/dblp/adj_time_list.pickle', 'rb') as handle:\n",
    "#with open('data/fb/adj_time_list.pickle', 'rb') as handle:\n",
    "with open('data/enron10/adj_time_list.pickle', 'rb') as handle:\n",
    "    adj_time_list = pickle.load(handle,encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c7ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_auc(recons_edges, true_edges):\n",
    "    predict_graph = recons_edges\n",
    "    predict_edges = np.array(predict_graph)\n",
    "    return average_precision_score(true_edges, predict_edges), roc_auc_score(true_edges, predict_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5757eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reconstruction_graph(nn.Module):\n",
    "    \"\"\"给定类簇质心，更新节点嵌入\"\"\"\n",
    "    def __init__(self, ne, alpha=1.0):\n",
    "        super(reconstruction_graph, self).__init__()\n",
    "        NE = ne\n",
    "        self.alpha = alpha\n",
    "        self.nodes_embedding = Parameter(NE)\n",
    "        \n",
    "    def forward(self, cc):\n",
    "        CC = cc\n",
    "        # 计算每个节点对每个类簇的注意力分数（以节点为单位）\n",
    "        norm_squared = torch.sum((self.nodes_embedding.unsqueeze(1) - CC)**2, 2)   # 节点到质心的距离\n",
    "        numerator = 1.0 / (1.0 + (norm_squared / self.alpha))\n",
    "        power = float(self.alpha + 1) / 2    # 计算幂\n",
    "        numerator = numerator**power    # 所有节点\n",
    "        soft_assignments = (numerator.t() / torch.sum(numerator, 1)).t() #soft assignment using t-distribution\n",
    "        # torch.sum(numerator, 1) : 对每一行进行相加\n",
    "        \n",
    "        # 计算节点之间的类簇相似性（余弦相似性）：值越大越有利于边的形成\n",
    "        prod = torch.mm(soft_assignments, soft_assignments.t())#分子\n",
    "        norm = torch.norm(soft_assignments,p=2,dim=1).unsqueeze(0)#分母\n",
    "        clusters_similar = prod.div(torch.mm(norm.t(),norm))\n",
    "        \n",
    "        # 计算节点之间的距离 ： 值越小越有利于边的形成\n",
    "        nodes_distance = torch.norm(self.nodes_embedding[:, None]-self.nodes_embedding, dim=2, p=2)\n",
    "        nodes_distance = torch.div(nodes_distance, torch.max(nodes_distance))\n",
    "        \n",
    "        # 计算边的形成概率\n",
    "        distance_similar = torch.div(beta*nodes_distance, clusters_similar)      \n",
    "        nodes_similar = torch.exp(-distance_similar)\n",
    "        \n",
    "        return nodes_similar\n",
    "\n",
    "class update_nodes_embedding(nn.Module):\n",
    "    def __init__(self, ne):\n",
    "        super(update_nodes_embedding, self).__init__()\n",
    "        NE = ne\n",
    "        self.reconstruction_module = reconstruction_graph(NE)     # 更新节点嵌入\n",
    "        self.optimizer = torch.optim.SGD(params=self.reconstruction_module.parameters(), lr=0.4, momentum=0.9)\n",
    "        #self.loss_function = torch.nn.L1Loss(reduction='sum')\n",
    "        self.loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, g, cc, edge_train, edge_test):\n",
    "        CC = cc\n",
    "        GP = g\n",
    "        self.reconstruction_module.train()\n",
    "        for epoch in range(5):\n",
    "            self.optimizer.zero_grad()\n",
    "            graph_reconstruction = self.reconstruction_module(CC)\n",
    "            graph_train = torch.take(graph_reconstruction, edge_train)\n",
    "            loss = self.loss_function(g, graph_train)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            #print(f'Epoch: {epoch:02d}, Loss: {loss.item():.4f}')\n",
    "        recons_test_edges = torch.take(graph_reconstruction, edge_test).detach()\n",
    "        ap, auc = predict_auc(recons_test_edges, test_edge)\n",
    "        return self.reconstruction_module.nodes_embedding.detach()\n",
    "    \n",
    "####################################### 以最大化预测效果为目标的类型质心更新 ########################################\n",
    "class ClusteringLayer(nn.Module):\n",
    "    \"\"\"给定节点嵌入，更新类簇质心\"\"\"\n",
    "    def __init__(self, cc, alpha=1.0):\n",
    "        super(ClusteringLayer, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.cluster_centers = Parameter(cc)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        NE = x\n",
    "        # 计算每个节点对每个类簇的注意力分数（以节点为单位）\n",
    "        norm_squared = torch.sum((NE.unsqueeze(1) - self.cluster_centers)**2, 2)   # 节点到质心的距离\n",
    "        numerator = 1.0 / (1.0 + (norm_squared / self.alpha))\n",
    "        power = float(self.alpha + 1) / 2    # 计算幂\n",
    "        numerator = numerator**power    # 所有节点\n",
    "        soft_assignments = (numerator.t() / torch.sum(numerator, 1)).t() #soft assignment using t-distribution\n",
    "        # torch.sum(numerator, 1) : 对每一行进行相加\n",
    "        \n",
    "        # 计算节点之间的类簇相似性（余弦相似性）：值越大越有利于边的形成\n",
    "        prod = torch.mm(soft_assignments, soft_assignments.t())#分子\n",
    "        norm = torch.norm(soft_assignments,p=2,dim=1).unsqueeze(0)#分母\n",
    "        clusters_similar = prod.div(torch.mm(norm.t(),norm))\n",
    "        \n",
    "        # 计算节点之间的距离 ： 值越小越有利于边的形成\n",
    "        nodes_distance = torch.norm(NE[:, None]-NE, dim=2, p=2)\n",
    "        nodes_distance = torch.div(nodes_distance, torch.max(nodes_distance))\n",
    "        \n",
    "        # 计算边的形成概率\n",
    "        distance_similar = torch.div(beta*nodes_distance, clusters_similar)      \n",
    "        nodes_similar = torch.exp(-distance_similar)\n",
    "        \n",
    "        return nodes_similar\n",
    "\n",
    "def find_cluster_centers(ne, ini_cc, g, edge_train, edge_test):        \n",
    "    NE = ne\n",
    "    CC = ini_cc\n",
    "    clusteringlayer = ClusteringLayer(CC)\n",
    "    optimizer = torch.optim.SGD(params=clusteringlayer.parameters(), lr=0.4, momentum=0.9)\n",
    "    #loss_function = torch.nn.L1Loss(reduction='sum')\n",
    "    loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        graph_recons = clusteringlayer(NE)\n",
    "        graph_train = torch.take(graph_recons, edge_train)\n",
    "        loss = loss_function(g, graph_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(f'Epoch: {epoch:02d}, Loss: {loss.item():.4f}')     \n",
    "    recons_test_edges = torch.take(graph_recons, edge_test).detach()\n",
    "    ap, auc = predict_auc(recons_test_edges, test_edge)\n",
    "    return clusteringlayer.cluster_centers.detach(), graph_recons.detach(), ap, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8796181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(graph_np, nodes_number):\n",
    "\n",
    "    posi_edge = np.argwhere(graph_np == 1)\n",
    "    posi_edge = posi_edge[posi_edge[:,0]<posi_edge[:,1]]         # 只取左上角矩阵\n",
    "    posi_edge_number = posi_edge.shape[0]\n",
    "    \n",
    "    nega_edge = np.argwhere(graph_np == 0) \n",
    "    nega_edge = nega_edge[nega_edge[:,0]<nega_edge[:,1]]     # 只取左上角矩阵\n",
    "    \n",
    "    #positive_index = np.random.choice(range(posi_edge_number),int(posi_edge_number*0.9),replace=False)\n",
    "    #choose_positive = posi_edge[positive_index]\n",
    "    choose_positive = posi_edge\n",
    "    #posi_not_choose = np.setdiff1d(range(posi_edge_number), positive_index)\n",
    "    #test_positive = posi_edge[posi_not_choose]\n",
    "    test_positive = posi_edge\n",
    "    \n",
    "    negative_index = np.random.choice(range(nega_edge.shape[0]),posi_edge_number*6,replace=False)\n",
    "    choose_negative = nega_edge[negative_index]\n",
    "    nega_not_choose = np.setdiff1d(range(nega_edge.shape[0]), negative_index)\n",
    "    test_nega_index = np.random.choice(range(nega_edge.shape[0]),posi_edge_number,replace=False)\n",
    "    test_negative = nega_edge[test_nega_index]\n",
    "    #test_negative = nega_edge\n",
    "    \n",
    "    train_posi = [list(choose_positive[i]) for i in range(len(choose_positive))]\n",
    "    train_nega = [list(choose_negative[i]) for i in range(len(choose_negative))]\n",
    "    train_index = train_posi + train_nega\n",
    "    train_mask = [train_index[i][0]*nodes_number+train_index[i][1] for i in range(len(train_index))]\n",
    "    train_mask = torch.tensor(train_mask)\n",
    "    \n",
    "    test_posi = [list(test_positive[i]) for i in range(len(test_positive))]\n",
    "    test_nega = [list(test_negative[i]) for i in range(len(test_negative))]\n",
    "    test_index = test_posi + test_nega\n",
    "    test_mask = [test_index[i][0]*nodes_number+test_index[i][1] for i in range(len(test_index))]\n",
    "    test_mask = torch.tensor(test_mask)\n",
    "    \n",
    "    graph_tensor = torch.from_numpy(graph_np).float()\n",
    "    train_edge = torch.take(graph_tensor, train_mask)\n",
    "    test_edge = np.array(torch.take(graph_tensor, test_mask))\n",
    "    \n",
    "    return train_edge, test_edge, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6141e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 8\n",
    "n_clusters = 12\n",
    "beta = 4.5\n",
    "LR = 0.01\n",
    "MO = 0.95\n",
    "NUM_EPOCH = 70\n",
    "alpha = 1.0\n",
    "nodes_number = 184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "543273d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9108414726912174\n",
      "auc :  0.8933837429111531\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9950017032600093\n",
      "auc :  0.9958412098298677\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9945272718229997\n",
      "auc :  0.9956143667296786\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9944685599202098\n",
      "auc :  0.9956143667296786\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9950073478872224\n",
      "auc :  0.995992438563327\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9946179324903636\n",
      "auc :  0.995765595463138\n"
     ]
    }
   ],
   "source": [
    "graph = adj_time_list[0].toarray()\n",
    "train_edge, test_edge, train_mask, test_mask = get_graph(graph, nodes_number)\n",
    "    \n",
    "ini_embedding = Embedding(nodes_number, embedding_dim, sparse=True)      # 时刻为0时给定随机的嵌入\n",
    "raw_nodes_embedding = ini_embedding.weight.detach()   # t=0时刻的节点嵌入\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(raw_nodes_embedding)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "raw_cluster_centers = torch.tensor(cluster_centers, dtype=torch.float)    # t=0时刻的类簇质心\n",
    "\n",
    "for module_epoch in range(100):\n",
    "    update_nodes_module = update_nodes_embedding(raw_nodes_embedding)\n",
    "    raw_nodes_embedding = update_nodes_module(train_edge, raw_cluster_centers, train_mask, test_mask)\n",
    "    raw_cluster_centers, LINK_PROB, ap, auc = find_cluster_centers(raw_nodes_embedding, raw_cluster_centers, train_edge, train_mask, test_mask)\n",
    "\n",
    "    if module_epoch%20==0 or module_epoch==99:\n",
    "        print(\"######################### module_epoch ： %d ##########################\"%module_epoch)\n",
    "        print(\"ap : \",ap)\n",
    "        print(\"auc : \",auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4693e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(raw_cluster_centers.numpy()).to_excel(\"Enron/Enron_cluster_centers.xlsx\")\n",
    "pd.DataFrame(raw_nodes_embedding.numpy()).to_excel(\"Enron/Enron_0_nodes_embedding.xlsx\")\n",
    "pd.DataFrame(LINK_PROB.numpy()).to_excel(\"Enron/Enron_0_link_prob.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31b3cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# TIME REGULAR TO GET NODES EMBEDDING ##########################################\n",
    "def computer_clustering(nodes_embedding, CC):\n",
    "    norm_squared = torch.sum((nodes_embedding.unsqueeze(1) - CC)**2, 2)   \n",
    "    numerator = 1.0 / (1.0 + (norm_squared / alpha))\n",
    "    power = float(alpha + 1) / 2    \n",
    "    numerator = numerator**power    \n",
    "    soft_assignments = (numerator.t() / torch.sum(numerator, 1)).t() #soft assignment using t-distribution\n",
    "    return soft_assignments\n",
    "\n",
    "class reconstruction_graph(nn.Module):\n",
    "    \"\"\"给定类簇质心，更新节点嵌入\"\"\"\n",
    "    def __init__(self, NE, alpha=1.0):\n",
    "        super(reconstruction_graph, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.nodes_embedding = Parameter(NE)\n",
    "        \n",
    "    def forward(self, CC):\n",
    "        # 计算每个节点对每个类簇的注意力分数（以节点为单位）\n",
    "        norm_squared = torch.sum((self.nodes_embedding.unsqueeze(1) - CC)**2, 2)   # 节点到质心的距离\n",
    "        numerator = 1.0 / (1.0 + (norm_squared / self.alpha))\n",
    "        power = float(self.alpha + 1) / 2    # 计算幂\n",
    "        numerator = numerator**power    # 所有节点\n",
    "        soft_assignments = (numerator.t() / torch.sum(numerator, 1)).t() #soft assignment using t-distribution\n",
    "        # torch.sum(numerator, 1) : 对每一行进行相加\n",
    "        \n",
    "        # 计算节点之间的类簇相似性（余弦相似性）：值越大越有利于边的形成\n",
    "        prod = torch.mm(soft_assignments, soft_assignments.t())#分子\n",
    "        norm = torch.norm(soft_assignments,p=2,dim=1).unsqueeze(0)#分母\n",
    "        clusters_similar = prod.div(torch.mm(norm.t(),norm))\n",
    "        \n",
    "        # 计算节点之间的距离 ： 值越小越有利于边的形成\n",
    "        nodes_distance = torch.norm(self.nodes_embedding[:, None]-self.nodes_embedding, dim=2, p=2)\n",
    "        nodes_distance = torch.div(nodes_distance, torch.max(nodes_distance))\n",
    "        \n",
    "        # 计算边的形成概率\n",
    "        distance_similar = torch.div(beta*nodes_distance, clusters_similar)      \n",
    "        nodes_similar = torch.exp(-distance_similar)\n",
    "        \n",
    "        return nodes_similar, soft_assignments\n",
    "\n",
    "class time_regular(nn.Module):\n",
    "    def __init__(self, NE):\n",
    "        super(time_regular, self).__init__()\n",
    "        self.reconstruction_module = reconstruction_graph(NE)     # 更新节点嵌入\n",
    "        self.optimizer = torch.optim.SGD(params=self.reconstruction_module.parameters(), lr=0.4, momentum=0.95)\n",
    "        #self.loss_function = torch.nn.L1Loss(reduction='sum')\n",
    "        self.loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, g, CC, edge_train, edge_test, history_clustering):\n",
    "        self.reconstruction_module.train()\n",
    "        for epoch in range(100):\n",
    "            self.optimizer.zero_grad()\n",
    "            graph_reconstruction, clustering = self.reconstruction_module(CC)\n",
    "            graph_train = torch.take(graph_reconstruction, edge_train)\n",
    "            loss = self.loss_function(g, graph_train)+ETA*self.loss_function(clustering, history_clustering)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            #print(f'Epoch: {epoch:02d}, Loss: {loss.item():.4f}')\n",
    "            with torch.no_grad():\n",
    "                if epoch%20==0 or epoch==99:\n",
    "                    recons_test_edges = torch.take(graph_reconstruction, edge_test).detach()\n",
    "                    ap, auc = predict_auc(recons_test_edges, test_edge)\n",
    "                    print(\"######################### module_epoch ： %d ##########################\"%epoch)\n",
    "                    print(\"ap : \",ap)\n",
    "                    print(\"auc : \",auc)\n",
    "                \n",
    "        return self.reconstruction_module.nodes_embedding.detach(), graph_reconstruction.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffd56ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW GET THE NODES EMBEDDING OF TIME 1\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9419176474341062\n",
      "auc :  0.945266272189349\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.964812993037447\n",
      "auc :  0.9707429322813939\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9601272369821501\n",
      "auc :  0.9721400394477319\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9656841139284813\n",
      "auc :  0.9737836949375411\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9671351511410957\n",
      "auc :  0.9752218934911243\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9666176686089288\n",
      "auc :  0.9759204470742932\n",
      "NOW GET THE NODES EMBEDDING OF TIME 2\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9202412518219426\n",
      "auc :  0.8948691705233179\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9560534809330864\n",
      "auc :  0.9315370238519046\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9677280603037242\n",
      "auc :  0.9467782128871484\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9708606649624023\n",
      "auc :  0.950249199003204\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9732949233714553\n",
      "auc :  0.9552331790672838\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9766426761053572\n",
      "auc :  0.9614409042363828\n",
      "NOW GET THE NODES EMBEDDING OF TIME 3\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9589965319741828\n",
      "auc :  0.9556596754408896\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9742241264469572\n",
      "auc :  0.9703794566550704\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9790395830426564\n",
      "auc :  0.976009974059402\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9833305373204514\n",
      "auc :  0.9797904643165959\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9837681556554183\n",
      "auc :  0.980353516057029\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9845783266055701\n",
      "auc :  0.9812182026584086\n",
      "NOW GET THE NODES EMBEDDING OF TIME 4\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.942070693305202\n",
      "auc :  0.9368933553884087\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.968963286702719\n",
      "auc :  0.9629537155519491\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9700876527354743\n",
      "auc :  0.9671982772073281\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9771429522844424\n",
      "auc :  0.9735963297025686\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9794107206355915\n",
      "auc :  0.9766861209075871\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9820874303271134\n",
      "auc :  0.9797447021004339\n",
      "NOW GET THE NODES EMBEDDING OF TIME 5\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9070194647498917\n",
      "auc :  0.914247063011748\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9363258239291911\n",
      "auc :  0.941374154503382\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9418458375364414\n",
      "auc :  0.9487219651121396\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9464171135108015\n",
      "auc :  0.9512281950872197\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9518881077663318\n",
      "auc :  0.9554147383410466\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9541498804343937\n",
      "auc :  0.9566963332146672\n",
      "NOW GET THE NODES EMBEDDING OF TIME 6\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9339852009402411\n",
      "auc :  0.9336111111111111\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9635866326284172\n",
      "auc :  0.9644791666666666\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9658430212526264\n",
      "auc :  0.9678645833333334\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9741736438096686\n",
      "auc :  0.9728472222222223\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9739608747904525\n",
      "auc :  0.9757465277777778\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9758388084397459\n",
      "auc :  0.9775694444444445\n",
      "NOW GET THE NODES EMBEDDING OF TIME 7\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9381641841710725\n",
      "auc :  0.9297755003202675\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9610810056912169\n",
      "auc :  0.9542564326891533\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9701379484370505\n",
      "auc :  0.9637863425455794\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9765467136008257\n",
      "auc :  0.970988454748551\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9812730997095925\n",
      "auc :  0.9781124529363059\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9828793243326734\n",
      "auc :  0.9792216719523816\n",
      "NOW GET THE NODES EMBEDDING OF TIME 8\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9644029070462217\n",
      "auc :  0.9641482715535193\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9611474211347051\n",
      "auc :  0.9738775510204081\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9790977123332468\n",
      "auc :  0.9826405664306539\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.971872685333665\n",
      "auc :  0.9815743440233236\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9720470934344186\n",
      "auc :  0.9824406497292795\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9781051131319056\n",
      "auc :  0.9837734277384423\n",
      "NOW GET THE NODES EMBEDDING OF TIME 9\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9543312956241963\n",
      "auc :  0.9494915613304145\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9733978637382956\n",
      "auc :  0.9724066097027046\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9869876495629262\n",
      "auc :  0.9841995621778123\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9881914510188937\n",
      "auc :  0.9855589294541347\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9885047801403612\n",
      "auc :  0.9864945978391356\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9894006417264684\n",
      "auc :  0.9876774239107408\n",
      "NOW GET THE NODES EMBEDDING OF TIME 10\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9298134172433368\n",
      "auc :  0.9110746791791509\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9687518344255702\n",
      "auc :  0.9646107750579455\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9794306044068584\n",
      "auc :  0.9749137882299734\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9817351207825696\n",
      "auc :  0.9783339928769291\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9851654042238295\n",
      "auc :  0.9828141783029001\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9867175178374791\n",
      "auc :  0.9854853298660184\n"
     ]
    }
   ],
   "source": [
    "ETA = 2.0\n",
    "CLUSTER_CENTERS = copy.deepcopy(raw_cluster_centers)\n",
    "MEMORY = torch.zeros((nodes_number, nodes_number, len(adj_time_list)-1))\n",
    "MEMORY[:,:,0] = LINK_PROB   # store module\n",
    "\n",
    "for i in range(1, len(adj_time_list)):\n",
    "    print(\"NOW GET THE NODES EMBEDDING OF TIME %d\"%i)\n",
    "    \n",
    "    #NEWLY_EMBEDDING = copy.deepcopy(raw_nodes_embedding)\n",
    "    HISTORY_CLUSTERING = computer_clustering(raw_nodes_embedding, CLUSTER_CENTERS)\n",
    "    \n",
    "    graph = adj_time_list[i].toarray()\n",
    "    train_edge, test_edge, train_mask, test_mask = get_graph(graph, nodes_number)\n",
    "    \n",
    "    #raw_nodes_embedding = NEWLY_EMBEDDING\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(raw_nodes_embedding)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    raw_cluster_centers = torch.tensor(cluster_centers, dtype=torch.float)    # t=0时刻的类簇质心\n",
    "\n",
    "    update_nodes_module = time_regular(raw_nodes_embedding)\n",
    "    raw_nodes_embedding, link_prob = update_nodes_module(train_edge, CLUSTER_CENTERS, train_mask, test_mask, HISTORY_CLUSTERING)\n",
    "            \n",
    "    if i!=len(adj_time_list)-1:\n",
    "        MEMORY[:,:,i] = link_prob\n",
    "    pd.DataFrame(raw_nodes_embedding.numpy()).to_excel(\"Enron/Enron_%d_nodes_embedding.xlsx\"%i)\n",
    "    pd.DataFrame(link_prob.numpy()).to_excel(\"Enron/Enron_%d_link_prob.xlsx\"%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4da02d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINK DETECTION\n",
    "def get_detection_graph(graph_np, nodes_number):\n",
    "    posi_edge = np.argwhere(graph_np == 1)\n",
    "    posi_edge = posi_edge[posi_edge[:,0]<posi_edge[:,1]]         # 只取左上角矩阵\n",
    "    posi_edge_number = posi_edge.shape[0]\n",
    "    \n",
    "    nega_edge = np.argwhere(graph_np == 0) \n",
    "    nega_edge = nega_edge[nega_edge[:,0]<nega_edge[:,1]]     # 只取左上角矩阵\n",
    "    \n",
    "    positive_index = np.random.choice(range(posi_edge_number),int(posi_edge_number*0.9),replace=False)\n",
    "    choose_positive = posi_edge[positive_index]\n",
    "    posi_not_choose = np.setdiff1d(range(posi_edge_number), positive_index)\n",
    "    test_positive = posi_edge[posi_not_choose]\n",
    "    \n",
    "    negative_index = np.random.choice(range(nega_edge.shape[0]),int(posi_edge_number*0.9)*4,replace=False)\n",
    "    choose_negative = nega_edge[negative_index]\n",
    "    nega_not_choose = np.setdiff1d(range(nega_edge.shape[0]), negative_index)\n",
    "    test_nega_index = np.random.choice(nega_not_choose,len(posi_not_choose),replace=False)\n",
    "    test_negative = nega_edge[test_nega_index]\n",
    "    \n",
    "    train_posi = [list(choose_positive[i]) for i in range(len(choose_positive))]\n",
    "    train_nega = [list(choose_negative[i]) for i in range(len(choose_negative))]\n",
    "    train_index = train_posi + train_nega\n",
    "    train_mask = [train_index[i][0]*nodes_number+train_index[i][1] for i in range(len(train_index))]\n",
    "    train_mask = torch.tensor(train_mask)\n",
    "    \n",
    "    test_posi = [list(test_positive[i]) for i in range(len(test_positive))]\n",
    "    test_nega = [list(test_negative[i]) for i in range(len(test_negative))]\n",
    "    test_index = test_posi + test_nega\n",
    "    test_mask = [test_index[i][0]*nodes_number+test_index[i][1] for i in range(len(test_index))]\n",
    "    test_mask = torch.tensor(test_mask)\n",
    "    \n",
    "    graph_tensor = torch.from_numpy(graph_np).float()\n",
    "    train_edge = torch.take(graph_tensor, train_mask)\n",
    "    test_edge = np.array(torch.take(graph_tensor, test_mask))\n",
    "    \n",
    "    return train_edge, test_edge, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bb24d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_clustering(nodes_embedding, CC):\n",
    "    norm_squared = torch.sum((nodes_embedding.unsqueeze(1) - CC)**2, 2)   \n",
    "    numerator = 1.0 / (1.0 + (norm_squared / alpha))\n",
    "    power = float(alpha + 1) / 2    \n",
    "    numerator = numerator**power    \n",
    "    soft_assignments = (numerator.t() / torch.sum(numerator, 1)).t() #soft assignment using t-distribution\n",
    "    return soft_assignments\n",
    "\n",
    "class reconstruction_graph(nn.Module):\n",
    "    \"\"\"给定类簇质心，更新节点嵌入\"\"\"\n",
    "    def __init__(self, NE, alpha=1.0):\n",
    "        super(reconstruction_graph, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.nodes_embedding = Parameter(NE)\n",
    "        self.linear = torch.nn.Linear(w+1, 1)\n",
    "        \n",
    "    def forward(self, CC):\n",
    "        # 计算每个节点对每个类簇的注意力分数（以节点为单位）\n",
    "        norm_squared = torch.sum((self.nodes_embedding.unsqueeze(1) - CC)**2, 2)   # 节点到质心的距离\n",
    "        numerator = 1.0 / (1.0 + (norm_squared / self.alpha))\n",
    "        power = float(self.alpha + 1) / 2    # 计算幂\n",
    "        numerator = numerator**power    # 所有节点\n",
    "        soft_assignments = (numerator.t() / torch.sum(numerator, 1)).t() #soft assignment using t-distribution\n",
    "        # torch.sum(numerator, 1) : 对每一行进行相加\n",
    "        \n",
    "        # 计算节点之间的类簇相似性（余弦相似性）：值越大越有利于边的形成\n",
    "        prod = torch.mm(soft_assignments, soft_assignments.t())#分子\n",
    "        norm = torch.norm(soft_assignments,p=2,dim=1).unsqueeze(0)#分母\n",
    "        clusters_similar = prod.div(torch.mm(norm.t(),norm))\n",
    "        \n",
    "        # 计算节点之间的距离 ： 值越小越有利于边的形成\n",
    "        nodes_distance = torch.norm(self.nodes_embedding[:, None]-self.nodes_embedding, dim=2, p=2)\n",
    "        nodes_distance = torch.div(nodes_distance, torch.max(nodes_distance))\n",
    "        \n",
    "        # 计算边的形成概率\n",
    "        distance_similar = torch.div(beta*nodes_distance, clusters_similar)      \n",
    "        link_prob = torch.exp(-distance_similar)\n",
    "    \n",
    "        return link_prob, soft_assignments\n",
    "\n",
    "class link_detection(nn.Module):\n",
    "    def __init__(self, NE):\n",
    "        super(link_detection, self).__init__()\n",
    "        self.reconstruction_module = reconstruction_graph(NE)     # 更新节点嵌入\n",
    "        self.optimizer = torch.optim.SGD(params=self.reconstruction_module.parameters(), lr=0.1, momentum=0.95)\n",
    "        self.loss_function = torch.nn.MSELoss(reduction='mean')\n",
    "        #self.cluster_loss_function = torch.nn.MSELoss(reduction='mean')\n",
    "        #self.linear = torch.nn.Linear(w+1, 1)\n",
    "        #self.linear_optimizer = torch.optim.SGD(params=self.linear.parameters(), lr=0.1, momentum=0.9)\n",
    "        \n",
    "    def forward(self, g, CC, edge_train, edge_test, history_clustering, history_link_prob):\n",
    "        self.reconstruction_module.train()\n",
    "        for epoch in range(200):\n",
    "            self.optimizer.zero_grad()\n",
    "            #self.linear_optimizer.zero_grad()\n",
    "            link_prob, clustering = self.reconstruction_module(CC)\n",
    "            link_prob = link_prob.unsqueeze(2)\n",
    "            #graph_reconstruction = self.linear(torch.cat((link_prob.long(), history_link_prob), dim=2))\n",
    "            graph_reconstruction = self.reconstruction_module.linear(torch.cat((link_prob.long(), history_link_prob), dim=2))\n",
    "            graph_train = torch.take(graph_reconstruction, edge_train.long())\n",
    "            loss = self.loss_function(g, graph_train)+ETA*self.loss_function(clustering, history_clustering)\n",
    "            loss.backward()\n",
    "            #self.linear_optimizer.step()\n",
    "            self.optimizer.step()\n",
    "            #print(f'Epoch: {epoch:02d}, Loss: {loss.item():.4f}')\n",
    "            #print(graph_reconstruction)\n",
    "            if epoch%20==0 or epoch==99:\n",
    "                print(\"######################### module_epoch ： %d ##########################\"%epoch)\n",
    "                with torch.no_grad():\n",
    "                    recons_test_edges = torch.take(graph_reconstruction, edge_test.long()).detach()\n",
    "                    predict_edges = np.array(recons_test_edges)\n",
    "                    ap = average_precision_score(test_edge, predict_edges)\n",
    "                    auc = roc_auc_score(test_edge, predict_edges)\n",
    "                    print(\"ap : \",ap)\n",
    "                    print(\"auc : \",auc)\n",
    "            \n",
    "        return self.reconstruction_module.nodes_embedding.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c48ac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW GET THE NODES EMBEDDING OF TIME 8\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.43302528204443747\n",
      "auc :  0.1488\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9732444444444445\n",
      "auc :  0.9664\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9815029415029415\n",
      "auc :  0.9808\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9719865659244971\n",
      "auc :  0.9712\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9663002750986182\n",
      "auc :  0.9663999999999999\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9693026591879587\n",
      "auc :  0.9696\n",
      "######################### module_epoch ： 100 ##########################\n",
      "ap :  0.9693026591879587\n",
      "auc :  0.9696\n",
      "######################### module_epoch ： 120 ##########################\n",
      "ap :  0.9703754561228246\n",
      "auc :  0.9696\n",
      "######################### module_epoch ： 140 ##########################\n",
      "ap :  0.9718421227894912\n",
      "auc :  0.9712000000000001\n",
      "######################### module_epoch ： 160 ##########################\n",
      "ap :  0.9703754561228246\n",
      "auc :  0.9696\n",
      "######################### module_epoch ： 180 ##########################\n",
      "ap :  0.9698648023587975\n",
      "auc :  0.9696\n",
      "NOW GET THE NODES EMBEDDING OF TIME 9\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.985092784313322\n",
      "auc :  0.986111111111111\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9888242283079239\n",
      "auc :  0.9895833333333334\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9909075616412573\n",
      "auc :  0.9913194444444444\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9846867484367483\n",
      "auc :  0.984375\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9861112498612499\n",
      "auc :  0.986111111111111\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9861112498612499\n",
      "auc :  0.986111111111111\n",
      "######################### module_epoch ： 100 ##########################\n",
      "ap :  0.9861112498612499\n",
      "auc :  0.986111111111111\n",
      "######################### module_epoch ： 120 ##########################\n",
      "ap :  0.9861112498612499\n",
      "auc :  0.986111111111111\n",
      "######################### module_epoch ： 140 ##########################\n",
      "ap :  0.9861112498612499\n",
      "auc :  0.986111111111111\n",
      "######################### module_epoch ： 160 ##########################\n",
      "ap :  0.9893691001027958\n",
      "auc :  0.9895833333333333\n",
      "######################### module_epoch ： 180 ##########################\n",
      "ap :  0.9877718778805735\n",
      "auc :  0.9878472222222222\n",
      "NOW GET THE NODES EMBEDDING OF TIME 10\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.45028709595569394\n",
      "auc :  0.19890260631001372\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9684649280059905\n",
      "auc :  0.9615912208504801\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.970190876800072\n",
      "auc :  0.9643347050754458\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9741188243554704\n",
      "auc :  0.9698216735253772\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9721304830175794\n",
      "auc :  0.9670781893004115\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9729708191520332\n",
      "auc :  0.9684499314128943\n",
      "######################### module_epoch ： 100 ##########################\n",
      "ap :  0.9729708191520332\n",
      "auc :  0.9684499314128943\n",
      "######################### module_epoch ： 120 ##########################\n",
      "ap :  0.9730545416819925\n",
      "auc :  0.9684499314128943\n",
      "######################### module_epoch ： 140 ##########################\n",
      "ap :  0.9730545416819925\n",
      "auc :  0.9684499314128943\n",
      "######################### module_epoch ： 160 ##########################\n",
      "ap :  0.9730545416819925\n",
      "auc :  0.9684499314128943\n",
      "######################### module_epoch ： 180 ##########################\n",
      "ap :  0.9730545416819925\n",
      "auc :  0.9684499314128943\n"
     ]
    }
   ],
   "source": [
    "T = len(adj_time_list)\n",
    "w = T-3\n",
    "\n",
    "for i in range(T-3, T):\n",
    "    print(\"NOW GET THE NODES EMBEDDING OF TIME %d\"%i)\n",
    "    HISTORY_LINK_PROB = MEMORY[:,:,i-w:i]    # READ MEMORY\n",
    "\n",
    "    raw_nodes_embedding = pd.read_excel(\"Enron/Enron_%d_nodes_embedding.xlsx\"%(i-1)).iloc[:, 1:]\n",
    "    raw_nodes_embedding = torch.from_numpy(np.array(raw_nodes_embedding))\n",
    "    \n",
    "    HISTORY_CLUSTERING = computer_clustering(raw_nodes_embedding, CLUSTER_CENTERS)\n",
    "    \n",
    "    graph = adj_time_list[i].toarray()\n",
    "    train_edge, test_edge, train_mask, test_mask = get_detection_graph(graph, nodes_number)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(raw_nodes_embedding)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    raw_cluster_centers = torch.tensor(cluster_centers, dtype=torch.float)    # t=0时刻的类簇质心\n",
    "    \n",
    "    update_nodes_module = link_detection(raw_nodes_embedding)\n",
    "    raw_nodes_embedding = update_nodes_module(train_edge, CLUSTER_CENTERS, train_mask, test_mask, HISTORY_CLUSTERING, HISTORY_LINK_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c5ed3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_time_graph(graph_np, nodes_number):\n",
    "\n",
    "    posi_edge = np.argwhere(graph_np == 1)\n",
    "    posi_edge = posi_edge[posi_edge[:,0]<posi_edge[:,1]]         # 只取左上角矩阵\n",
    "    posi_edge_number = posi_edge.shape[0]\n",
    "    \n",
    "    nega_edge = np.argwhere(graph_np == 0) \n",
    "    nega_edge = nega_edge[nega_edge[:,0]<nega_edge[:,1]]     # 只取左上角矩阵\n",
    "    \n",
    "    test_positive = posi_edge\n",
    "    \n",
    "    test_nega_index = np.random.choice(range(nega_edge.shape[0]),posi_edge_number,replace=False)\n",
    "    test_negative = nega_edge[test_nega_index]\n",
    "\n",
    "    test_posi = [list(test_positive[i]) for i in range(len(test_positive))]\n",
    "    test_nega = [list(test_negative[i]) for i in range(len(test_negative))]\n",
    "    test_index = test_posi + test_nega\n",
    "    test_mask = [test_index[i][0]*nodes_number+test_index[i][1] for i in range(len(test_index))]\n",
    "    test_mask = torch.tensor(test_mask)\n",
    "    \n",
    "    graph_tensor = torch.from_numpy(graph_np).float()\n",
    "    test_edge = np.array(torch.take(graph_tensor, test_mask))\n",
    "    \n",
    "    return test_edge, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1dfb83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW GET THE NODES EMBEDDING OF TIME 8\n",
      "OrderedDict([('weight', tensor([[-0.2513, -0.3604, -0.1711, -0.3273,  0.2632,  0.3320,  0.2982]])), ('bias', tensor([-0.0810]))])\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.5290354487611812\n",
      "auc :  0.34045212392007373\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.94230853260999\n",
      "auc :  0.9458826102579325\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.944495723235767\n",
      "auc :  0.9418206814666689\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9465550186308131\n",
      "auc :  0.9486478463966005\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.947892644266608\n",
      "auc :  0.9477104782140011\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9457744419433296\n",
      "auc :  0.9479916886687809\n",
      "################## PREDICT NEXT GRAPH ##################\n",
      "ap :  0.9480572526680113\n",
      "auc :  0.9571012078300708\n",
      "OrderedDict([('weight', tensor([[ 0.0743,  0.0311,  0.3186,  0.2457, -0.0206,  0.1702,  0.4476]])), ('bias', tensor([-0.0159]))])\n",
      "NOW GET THE NODES EMBEDDING OF TIME 9\n",
      "OrderedDict([('weight', tensor([[ 0.1674, -0.2827,  0.1467,  0.1571,  0.2160,  0.0080, -0.0639]])), ('bias', tensor([-0.0978]))])\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.9036584572087448\n",
      "auc :  0.8782340691378592\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9561163782736433\n",
      "auc :  0.9580174927113703\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9556195245115482\n",
      "auc :  0.957167846730529\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9588114756838226\n",
      "auc :  0.9587505206164098\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9575510402396185\n",
      "auc :  0.9591170345689297\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9575223954300552\n",
      "auc :  0.9592003331945023\n",
      "################## PREDICT NEXT GRAPH ##################\n",
      "ap :  0.9652581867576504\n",
      "auc :  0.9590777487465574\n",
      "OrderedDict([('weight', tensor([[-0.0534, -0.0324,  0.0816,  0.1630,  0.3257,  0.0771,  0.6376]])), ('bias', tensor([-0.0206]))])\n",
      "NOW GET THE NODES EMBEDDING OF TIME 10\n",
      "OrderedDict([('weight', tensor([[ 0.1641,  0.2407, -0.0678,  0.0197, -0.1147, -0.1831, -0.1104]])), ('bias', tensor([-0.0538]))])\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.5284700166564449\n",
      "auc :  0.36577572205352726\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.941191011077088\n",
      "auc :  0.9454487677423912\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9407922642000821\n",
      "auc :  0.9455017301038063\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9399645384285502\n",
      "auc :  0.9462961655250336\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9420174851982168\n",
      "auc :  0.9457665419108818\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9411098621757751\n",
      "auc :  0.9467022102958831\n",
      "################## PREDICT NEXT GRAPH ##################\n",
      "ap :  0.948279643991772\n",
      "auc :  0.9479902764429872\n",
      "OrderedDict([('weight', tensor([[ 0.0901,  0.2272, -0.0777,  0.3217, -0.0919,  0.2297,  0.5972]])), ('bias', tensor([-0.0280]))])\n"
     ]
    }
   ],
   "source": [
    "############################################## LINK PREDICTION ##############################################\n",
    "predict_w = T-4\n",
    "\n",
    "class predict_next_graph(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(predict_next_graph, self).__init__()\n",
    "        self.linear = torch.nn.Linear(predict_w, 1)\n",
    "        \n",
    "    def forward(self, history_link_prob):\n",
    "        predicted_graph = self.linear(history_link_prob)\n",
    "        return predicted_graph\n",
    "\n",
    "predict_loss_function = torch.nn.MSELoss(reduction='mean')\n",
    "    \n",
    "for i in range(T-4, T-1):\n",
    "    print(\"NOW GET THE NODES EMBEDDING OF TIME %d\"%(i+1))\n",
    "    predict_modle = predict_next_graph()\n",
    "    print(predict_modle.linear.state_dict())\n",
    "    predict_optimizer = torch.optim.SGD(params=predict_modle.parameters(), lr=0.4, momentum=0.95)\n",
    "    \n",
    "    HISTORY_LINK = MEMORY[:,:,i-predict_w:i]    # READ MEMORY\n",
    "    \n",
    "    PREDICT_HISTORY_LINK = MEMORY[:,:,i+1-predict_w:i+1]    # READ MEMORY\n",
    "    \n",
    "    predict_graph = adj_time_list[i+1].toarray()\n",
    "    predict_test_edge, predict_test_mask = get_next_time_graph(predict_graph, nodes_number)\n",
    "    \n",
    "    train_graph = adj_time_list[i].toarray()\n",
    "    train_edge, test_edge, train_mask, test_mask = get_graph(train_graph, nodes_number)\n",
    "    \n",
    "    predict_modle.train()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        predict_optimizer.zero_grad()\n",
    "        predicted_link = predict_modle(HISTORY_LINK)\n",
    "        graph_train = torch.take(predicted_link, train_mask.long())\n",
    "        loss = predict_loss_function(train_edge, graph_train)\n",
    "        loss.backward()\n",
    "        predict_optimizer.step()\n",
    "        if epoch%20==0 or epoch==99:\n",
    "            print(\"######################### module_epoch ： %d ##########################\"%epoch)\n",
    "            with torch.no_grad():\n",
    "                recons_test_edges = torch.take(predicted_link, test_mask.long()).detach()\n",
    "                predict_edges = np.array(recons_test_edges)\n",
    "                ap = average_precision_score(test_edge, predict_edges)\n",
    "                auc = roc_auc_score(test_edge, predict_edges)\n",
    "                print(\"ap : \",ap)\n",
    "                print(\"auc : \",auc)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print(\"################## PREDICT NEXT GRAPH ##################\")\n",
    "        next_graph_predict = predict_modle(PREDICT_HISTORY_LINK)\n",
    "        linear_test_edges = torch.take(next_graph_predict, predict_test_mask.long()).detach()\n",
    "        linear_test_edges = np.array(linear_test_edges)\n",
    "        print(\"ap : \", average_precision_score(predict_test_edge, linear_test_edges))\n",
    "        print(\"auc : \", roc_auc_score(predict_test_edge, linear_test_edges))\n",
    "        \n",
    "    print(predict_modle.linear.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ff34ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_link(graph_index, nodes_number):\n",
    "    old_graph = adj_time_list[graph_index].toarray()\n",
    "    new_graph = adj_time_list[graph_index+1].toarray()\n",
    "    graph_np = new_graph - old_graph\n",
    "    graph_np[graph_np!=1]=0\n",
    "\n",
    "    posi_edge = np.argwhere(graph_np == 1)\n",
    "    posi_edge = posi_edge[posi_edge[:,0]<posi_edge[:,1]]         # 只取左上角矩阵\n",
    "    posi_edge_number = posi_edge.shape[0]\n",
    "    \n",
    "    nega_edge = np.argwhere(graph_np == 0) \n",
    "    nega_edge = nega_edge[nega_edge[:,0]<nega_edge[:,1]]     # 只取左上角矩阵\n",
    "    \n",
    "    test_positive = posi_edge\n",
    "    \n",
    "    test_nega_index = np.random.choice(range(nega_edge.shape[0]),posi_edge_number,replace=False)\n",
    "    test_negative = nega_edge[test_nega_index]\n",
    "\n",
    "    test_posi = [list(test_positive[i]) for i in range(len(test_positive))]\n",
    "    test_nega = [list(test_negative[i]) for i in range(len(test_negative))]\n",
    "    test_index = test_posi + test_nega\n",
    "    test_mask = [test_index[i][0]*nodes_number+test_index[i][1] for i in range(len(test_index))]\n",
    "    test_mask = torch.tensor(test_mask)\n",
    "    \n",
    "    graph_tensor = torch.from_numpy(graph_np).float()\n",
    "    test_edge = np.array(torch.take(graph_tensor, test_mask))\n",
    "    \n",
    "    return test_edge, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b7ccb843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW GET THE NODES EMBEDDING OF TIME 8\n",
      "OrderedDict([('weight', tensor([[-0.0135,  0.3022, -0.0173,  0.0136, -0.1330, -0.2126, -0.1314]])), ('bias', tensor([0.2442]))])\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.3724969562428556\n",
      "auc :  0.16880438688309457\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9538478583856305\n",
      "auc :  0.9461638207127122\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9362949384117102\n",
      "auc :  0.9279320095611554\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9548164537710752\n",
      "auc :  0.9471949257135716\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9516309148135685\n",
      "auc :  0.9448202596509866\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9525351482608914\n",
      "auc :  0.9458201190457592\n",
      "################## PREDICT NEXT GRAPH ##################\n",
      "ap :  0.8724061639179441\n",
      "auc :  0.9068040876932483\n",
      "OrderedDict([('weight', tensor([[0.0575, 0.0138, 0.1428, 0.1589, 0.2200, 0.1812, 0.5725]])), ('bias', tensor([-0.0483]))])\n",
      "NOW GET THE NODES EMBEDDING OF TIME 9\n",
      "OrderedDict([('weight', tensor([[ 0.1795, -0.3714,  0.0275, -0.1678, -0.2201,  0.0711, -0.2140]])), ('bias', tensor([0.0517]))])\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.3176910880824012\n",
      "auc :  0.053111203665139536\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.9637710635620983\n",
      "auc :  0.9624156601416076\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9607093388191738\n",
      "auc :  0.9544689712619742\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9670660143821528\n",
      "auc :  0.9658975426905456\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9668795173344952\n",
      "auc :  0.9653977509371096\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.9668057100100411\n",
      "auc :  0.9655476884631403\n",
      "################## PREDICT NEXT GRAPH ##################\n",
      "ap :  0.9169547887196589\n",
      "auc :  0.913643632690851\n",
      "OrderedDict([('weight', tensor([[0.0470, 0.0196, 0.0602, 0.1617, 0.2690, 0.2180, 0.6239]])), ('bias', tensor([-0.0396]))])\n",
      "NOW GET THE NODES EMBEDDING OF TIME 10\n",
      "OrderedDict([('weight', tensor([[-0.1950,  0.2290, -0.3645,  0.0902,  0.2330,  0.2845,  0.0774]])), ('bias', tensor([-0.2169]))])\n",
      "######################### module_epoch ： 0 ##########################\n",
      "ap :  0.8838161529903656\n",
      "auc :  0.8942518183744086\n",
      "######################### module_epoch ： 20 ##########################\n",
      "ap :  0.946378983535584\n",
      "auc :  0.9445484075983336\n",
      "######################### module_epoch ： 40 ##########################\n",
      "ap :  0.9471583386450914\n",
      "auc :  0.9449191441282395\n",
      "######################### module_epoch ： 60 ##########################\n",
      "ap :  0.9478524292966072\n",
      "auc :  0.9460313537179578\n",
      "######################### module_epoch ： 80 ##########################\n",
      "ap :  0.9494077101459226\n",
      "auc :  0.9470729468257891\n",
      "######################### module_epoch ： 99 ##########################\n",
      "ap :  0.948002810665288\n",
      "auc :  0.946013699597486\n",
      "################## PREDICT NEXT GRAPH ##################\n",
      "ap :  0.9089384119169323\n",
      "auc :  0.905716450090321\n",
      "OrderedDict([('weight', tensor([[ 0.1551,  0.0773,  0.0295,  0.3343, -0.1616,  0.2117,  0.6067]])), ('bias', tensor([-0.0184]))])\n"
     ]
    }
   ],
   "source": [
    "########################################### NEW LINK PREDICTION ##############################################\n",
    "predict_w = T-4\n",
    "\n",
    "class predict_next_graph(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(predict_next_graph, self).__init__()\n",
    "        self.linear = torch.nn.Linear(predict_w, 1)\n",
    "        \n",
    "    def forward(self, history_link_prob):\n",
    "        predicted_graph = self.linear(history_link_prob)\n",
    "        return predicted_graph\n",
    "\n",
    "predict_loss_function = torch.nn.MSELoss(reduction='mean')\n",
    "    \n",
    "for i in range(T-4, T-1):\n",
    "    print(\"NOW GET THE NODES EMBEDDING OF TIME %d\"%(i+1))\n",
    "    predict_modle = predict_next_graph()\n",
    "    print(predict_modle.linear.state_dict())\n",
    "    predict_optimizer = torch.optim.SGD(params=predict_modle.parameters(), lr=0.4, momentum=0.95)\n",
    "    \n",
    "    HISTORY_LINK = MEMORY[:,:,i-predict_w:i]    # READ MEMORY\n",
    "    \n",
    "    PREDICT_HISTORY_LINK = MEMORY[:,:,i+1-predict_w:i+1]    # READ MEMORY\n",
    "    \n",
    "    predict_graph = adj_time_list[i+1].toarray()\n",
    "    predict_test_edge, predict_test_mask = get_new_link(i, nodes_number)\n",
    "    \n",
    "    train_graph = adj_time_list[i].toarray()\n",
    "    train_edge, test_edge, train_mask, test_mask = get_graph(train_graph, nodes_number)\n",
    "    \n",
    "    predict_modle.train()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        predict_optimizer.zero_grad()\n",
    "        predicted_link = predict_modle(HISTORY_LINK)\n",
    "        graph_train = torch.take(predicted_link, train_mask.long())\n",
    "        loss = predict_loss_function(train_edge, graph_train)\n",
    "        loss.backward()\n",
    "        predict_optimizer.step()\n",
    "        if epoch%20==0 or epoch==99:\n",
    "            print(\"######################### module_epoch ： %d ##########################\"%epoch)\n",
    "            with torch.no_grad():\n",
    "                recons_test_edges = torch.take(predicted_link, test_mask.long()).detach()\n",
    "                predict_edges = np.array(recons_test_edges)\n",
    "                ap = average_precision_score(test_edge, predict_edges)\n",
    "                auc = roc_auc_score(test_edge, predict_edges)\n",
    "                print(\"ap : \",ap)\n",
    "                print(\"auc : \",auc)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print(\"################## PREDICT NEXT GRAPH ##################\")\n",
    "        next_graph_predict = predict_modle(PREDICT_HISTORY_LINK)\n",
    "        linear_test_edges = torch.take(next_graph_predict, predict_test_mask.long()).detach()\n",
    "        linear_test_edges = np.array(linear_test_edges)\n",
    "        print(\"ap : \", average_precision_score(predict_test_edge, linear_test_edges))\n",
    "        print(\"auc : \", roc_auc_score(predict_test_edge, linear_test_edges))\n",
    "        \n",
    "    print(predict_modle.linear.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95040d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
